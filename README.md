# BERT-Family
BERT-家族，NLP自然语言处理框架

![image](https://user-images.githubusercontent.com/36963108/209763122-bc1c553d-9044-4ea5-9895-4ab9c83fe821.png)


PPT介绍： http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/BERT%20train%20(v8).pdf


# 预训练的bert

![image](https://user-images.githubusercontent.com/36963108/209763536-6fd4b614-5500-4510-9961-69528e16446a.png)
- https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58
- https://www.jiqizhixin.com/articles/2019-08-26-16
- https://www.jianshu.com/p/fe94c54a4c05
- https://www.jianshu.com/p/7677e1a5813c
- https://www.jianshu.com/p/99ec2bf2ab6c
- https://www.jianshu.com/p/069fc8e05d79
- https://www.jianshu.com/p/e491c9486d17


# 参考

## 论文
1.  [Visualizing and measuring the geometry of BERT, 6 Jun 20](https://arxiv.org/pdf/1906.02715.pdf)19
2.  [Are 16 heads really better than one? May 2019](https://arxiv.org/pdf/1905.10650.pdf)
3.  [Can a machine finish your sentence? May 2019](https://arxiv.org/pdf/1905.07830.pdf)
4.  [BERT rediscovers classical NLP pipeline May 2019](https://arxiv.org/pdf/1905.05950.pdf)
5.  [What does BERT look at? An analysis of BERT’s attention, 11 June 2019](https://arxiv.org/pdf/1906.04341.pdf)
6.  [A structural probe for finding syntax in word representations April 2019](https://nlp.stanford.edu/pubs/hewitt2019structural.pdf)
7.  [BioBERT- a pre-trained biomedical language representation model for biomedical text mining, Jan 2019](https://arxiv.org/abs/1901.08746)
8.  [SciBERT: Pretrained Contextualized Embeddings for Scientific Text, March 2019](https://arxiv.org/abs/1903.10676)
9.  [ClinicalBert: Modeling Clinical Notes and Predicting Hospital Readmission, April 2019](https://arxiv.org/pdf/1904.05342.pdf)
10.  [Publicly available clinical BERT embeddings, April 2019](https://arxiv.org/pdf/1904.03323.pdf)
11.  [How multilingual is BERT June 2019](https://arxiv.org/pdf/1906.01502.pdf)
12.  [ERNIE: Enhanced Language Representation with Informative Entities, June 2019](https://arxiv.org/pdf/1905.07129.pdf)
13.  [Story ending prediction by transferable BERT, May 2019](https://arxiv.org/pdf/1905.07504.pdf)
14.  [VideoBERT: A Joint Model for Video and Language Representation Learning, April 2019](https://arxiv.org/pdf/1904.01766.pdf)
15.  [PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model, June 2019](https://arxiv.org/pdf/1906.02124.pdf)
16.  [DocBERT — Boer for document classification, April 2019](https://arxiv.org/pdf/1904.08398.pdf)
17.  [Pre-training of Graph Augmented Transformers for Medication Recommendation, June 2019](https://arxiv.org/pdf/1906.00346.pdf)
18.  [Attention is (not) all you need for common sense reasoning, May 2019](https://arxiv.org/pdf/1905.13497.pdf)

## 一些博客参考
- https://zhuanlan.zhihu.com/p/270338945
- https://www.projectpro.io/article/bert-nlp-model-explained/558
- https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23
- https://zhuanlan.zhihu.com/p/145119424
- https://onedreame.github.io/2020/10/31/bert%E5%AE%B6%E6%97%8F/
- https://blog.csdn.net/u013596454/article/details/120529965

## bert家族知识蒸馏

https://cloud.tencent.com/developer/article/1796814

## GPT庞大的家族系谱

- https://www.sohu.com/a/422574962_129720
- https://zhuanlan.zhihu.com/p/99953380

## VL-BERT视觉和语言表示的预训练模型

https://www.jianshu.com/p/7327fa83dca8

## 一些权重

*   **Google原版bert**: [https://github.com/google-research/bert](https://github.com/google-research/bert)
*   **brightmart版roberta**: [https://github.com/brightmart/roberta_zh](https://github.com/brightmart/roberta_zh)
*   **哈工大版roberta**: [https://github.com/ymcui/Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
*   **Google原版albert**<sup>[[例子]](https://github.com/bojone/bert4keras/issues/29#issuecomment-552188981)</sup>: [https://github.com/google-research/ALBERT](https://github.com/google-research/ALBERT)
*   **brightmart版albert**: [https://github.com/brightmart/albert_zh](https://github.com/brightmart/albert_zh)
*   **转换后的albert**: [https://github.com/bojone/albert_zh](https://github.com/bojone/albert_zh)
*   **华为的NEZHA**: [https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow)
*   **华为的NEZHA-GEN**: [https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow)
*   **自研语言模型**: [https://github.com/ZhuiyiTechnology/pretrained-models](https://github.com/ZhuiyiTechnology/pretrained-models)
*   **T5模型**: [https://github.com/google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)
*   **GPT_OpenAI**: [https://github.com/bojone/CDial-GPT-tf](https://github.com/bojone/CDial-GPT-tf)
*   **GPT2_ML**: [https://github.com/imcaspar/gpt2-ml](https://github.com/imcaspar/gpt2-ml)
*   **Google原版ELECTRA**: [https://github.com/google-research/electra](https://github.com/google-research/electra)
*   **哈工大版ELECTRA**: [https://github.com/ymcui/Chinese-ELECTRA](https://github.com/ymcui/Chinese-ELECTRA)
*   **CLUE版ELECTRA**: [https://github.com/CLUEbenchmark/ELECTRA](https://github.com/CLUEbenchmark/ELECTRA)
*   **LaBSE（多国语言BERT）**: [https://github.com/bojone/labse](https://github.com/bojone/labse)
*   **Chinese-GEN项目下的模型**: [https://github.com/bojone/chinese-gen](https://github.com/bojone/chinese-gen)
*   **T5.1.1**: [https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511](https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511)
*   **Multilingual T5**: [https://github.com/google-research/multilingual-t5/](https://github.com/google-research/multilingual-t5/)



[1] NLP将迎来黄金十年 https://www.msra.cn/zh-cn/news/executivebylines/tech-bylines-nlp

[2] a review of the recent history of nlp

[3] AIS：ACL2019进展报告

[4] ACL 主席周明：一起拥抱 ACL 和 NLP 的光明未来

[5] 自然语言处理中的语言模型预训练方法 https://www.jiqizhixin.com/articles/2018-10-22-3

[6] ELMO:Deep contextualized word representations

[7] ULMFiT：Universal Language Model Fine-tuning)

[8] SiATL：An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models

[9] BERT时代与后时代的NLP https://zhuanlan.zhihu.com/p/66676144

[10] GPT:Improving Language Understanding by Generative Pre-Training

[11] GPT2.0:Language Models are Unsupervised Multitask Learners

[12] Transformer:Attention is all you need

[13] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

[14] Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展 https://zhuanlan.zhihu.com/p/68446772

[15] MASS: Masked Sequence to Sequence Pre-training for Language Generation

[16] UNILM：Unified Language Model Pre-training for Natural Language Understanding and Generation

[17] ERNIE: Enhanced Representation through Knowledge Integration

[18] ERNIE: Enhanced Language Representation with Information Entities

[19] nndl：神经网络与深度学习

[20] MT-DNN：Multi-Task Deep Neural Net for NLU

[21] ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FOR LANGUAGE UNDERSTANDING

[22] 陈凯：https://www.zhihu.com/question/337827682/answer/768908184

[23] SpanBert：对 Bert 预训练的一次深度探索

[24] RoBERTa: A Robustly Optimized BERT Pretraining Approach

[25] ab他们创造了横扫NLP的XLNet：专访CMU博士杨植麟

[26] XLnet: Generalized Autoregressive Pretraining for Language Understanding

[27] Neural autoregressive distribution estimation

[28] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context

