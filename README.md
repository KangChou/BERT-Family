# BERT-Family
BERT-家族，NLP自然语言处理框架
- bert tensorflow版本：https://github.com/google-research/bert
- bert pytorch版本： https://github.com/codertimo/BERT-pytorch
- 中文BERT-wwm系列模型：https://github.com/ymcui/Chinese-BERT-wwm

bert相关论文汇总：
- https://github.com/tomohideshibata/BERT-related-papers
- https://github.com/Jiakui/awesome-bert

中文NLP资源库:
- https://github.com/fighting41love/funNLP
- 
![image](https://user-images.githubusercontent.com/36963108/209763122-bc1c553d-9044-4ea5-9895-4ab9c83fe821.png)



PPT介绍： http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/BERT%20train%20(v8).pdf

bert系列算法封装调用：
- https://github.com/bojone/bert4keras
- https://github.com/CyberZHG/keras-bert

bert任务：
- https://github.com/macanv/BERT-BiLSTM-CRF-NER
- https://github.com/terrifyzhao/bert-utils
- https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch
- https://github.com/WenRichard/KBQA-BERT
- https://github.com/kyzhouhzau/BERT-NER
- https://github.com/MaartenGr/KeyBERT
- https://github.com/yao8839836/kg-bert
- https://github.com/kamalkraj/BERT-NER
- https://github.com/lonePatient/BERT-NER-Pytorch
- https://github.com/Morizeyao/GPT2-Chinese
- https://github.com/ProHiryu/bert-chinese-ner
- https://github.com/Y1ran/NLP-BERT--ChineseVersion
- https://github.com/xmxoxo/BERT-train2deploy


# 预训练的bert

![image](https://user-images.githubusercontent.com/36963108/209763536-6fd4b614-5500-4510-9961-69528e16446a.png)
- https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58
- https://www.jiqizhixin.com/articles/2019-08-26-16
- https://github.com/utterworks/fast-bert
- https://codeantenna.com/a/8g5toAhu4v
- https://www.jianshu.com/p/fe94c54a4c05
- https://www.jianshu.com/p/7677e1a5813c
- https://www.jianshu.com/p/99ec2bf2ab6c
- https://www.jianshu.com/p/069fc8e05d79
- https://www.jianshu.com/p/e491c9486d17


# 参考

## 论文
1.  [Visualizing and measuring the geometry of BERT, 6 Jun 20](https://arxiv.org/pdf/1906.02715.pdf)19
2.  [Are 16 heads really better than one? May 2019](https://arxiv.org/pdf/1905.10650.pdf)
3.  [Can a machine finish your sentence? May 2019](https://arxiv.org/pdf/1905.07830.pdf)
4.  [BERT rediscovers classical NLP pipeline May 2019](https://arxiv.org/pdf/1905.05950.pdf)
5.  [What does BERT look at? An analysis of BERT’s attention, 11 June 2019](https://arxiv.org/pdf/1906.04341.pdf)
6.  [A structural probe for finding syntax in word representations April 2019](https://nlp.stanford.edu/pubs/hewitt2019structural.pdf)
7.  [BioBERT- a pre-trained biomedical language representation model for biomedical text mining, Jan 2019](https://arxiv.org/abs/1901.08746)
8.  [SciBERT: Pretrained Contextualized Embeddings for Scientific Text, March 2019](https://arxiv.org/abs/1903.10676)
9.  [ClinicalBert: Modeling Clinical Notes and Predicting Hospital Readmission, April 2019](https://arxiv.org/pdf/1904.05342.pdf)
10.  [Publicly available clinical BERT embeddings, April 2019](https://arxiv.org/pdf/1904.03323.pdf)
11.  [How multilingual is BERT June 2019](https://arxiv.org/pdf/1906.01502.pdf)
12.  [ERNIE: Enhanced Language Representation with Informative Entities, June 2019](https://arxiv.org/pdf/1905.07129.pdf)
13.  [Story ending prediction by transferable BERT, May 2019](https://arxiv.org/pdf/1905.07504.pdf)
14.  [VideoBERT: A Joint Model for Video and Language Representation Learning, April 2019](https://arxiv.org/pdf/1904.01766.pdf)
15.  [PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model, June 2019](https://arxiv.org/pdf/1906.02124.pdf)
16.  [DocBERT — Boer for document classification, April 2019](https://arxiv.org/pdf/1904.08398.pdf)
17.  [Pre-training of Graph Augmented Transformers for Medication Recommendation, June 2019](https://arxiv.org/pdf/1906.00346.pdf)
18.  [Attention is (not) all you need for common sense reasoning, May 2019](https://arxiv.org/pdf/1905.13497.pdf)

## 一些博客参考
- https://zhuanlan.zhihu.com/p/270338945
- https://www.projectpro.io/article/bert-nlp-model-explained/558
- https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23
- https://zhuanlan.zhihu.com/p/145119424
- https://onedreame.github.io/2020/10/31/bert%E5%AE%B6%E6%97%8F/
- https://blog.csdn.net/u013596454/article/details/120529965

## bert家族知识蒸馏

- https://cloud.tencent.com/developer/article/1796814
- https://github.com/MuQiuJun-AI/bert4pytorch

## GPT庞大的家族系谱

- https://www.sohu.com/a/422574962_129720
- https://zhuanlan.zhihu.com/p/99953380

## VL-BERT视觉和语言表示的预训练模型

https://www.jianshu.com/p/7327fa83dca8

## 一些权重

*   **Google原版bert**: [https://github.com/google-research/bert](https://github.com/google-research/bert)
*   **brightmart版roberta**: [https://github.com/brightmart/roberta_zh](https://github.com/brightmart/roberta_zh)
*   **哈工大版roberta**: [https://github.com/ymcui/Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
*   **Google原版albert**<sup>[[例子]](https://github.com/bojone/bert4keras/issues/29#issuecomment-552188981)</sup>: [https://github.com/google-research/ALBERT](https://github.com/google-research/ALBERT)
*   **brightmart版albert**: [https://github.com/brightmart/albert_zh](https://github.com/brightmart/albert_zh)
*   **转换后的albert**: [https://github.com/bojone/albert_zh](https://github.com/bojone/albert_zh)
*   **华为的NEZHA**: [https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow)
*   **华为的NEZHA-GEN**: [https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow)
*   **自研语言模型**: [https://github.com/ZhuiyiTechnology/pretrained-models](https://github.com/ZhuiyiTechnology/pretrained-models)
*   **T5模型**: [https://github.com/google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)
*   **GPT_OpenAI**: [https://github.com/bojone/CDial-GPT-tf](https://github.com/bojone/CDial-GPT-tf)
*   **GPT2_ML**: [https://github.com/imcaspar/gpt2-ml](https://github.com/imcaspar/gpt2-ml)
*   **Google原版ELECTRA**: [https://github.com/google-research/electra](https://github.com/google-research/electra)
*   **哈工大版ELECTRA**: [https://github.com/ymcui/Chinese-ELECTRA](https://github.com/ymcui/Chinese-ELECTRA)
*   **CLUE版ELECTRA**: [https://github.com/CLUEbenchmark/ELECTRA](https://github.com/CLUEbenchmark/ELECTRA)
*   **LaBSE（多国语言BERT）**: [https://github.com/bojone/labse](https://github.com/bojone/labse)
*   **Chinese-GEN项目下的模型**: [https://github.com/bojone/chinese-gen](https://github.com/bojone/chinese-gen)
*   **T5.1.1**: [https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511](https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511)
*   **Multilingual T5**: [https://github.com/google-research/multilingual-t5/](https://github.com/google-research/multilingual-t5/)
*   https://github.com/fighting41love/funNLP

## 其他参考
*   **[Transformer:Attention集大成者](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483679%26idx%3D1%26sn%3D26ec25fcb954332f2c21ca89c324278e%26chksm%3Dec3688d9db4101cf99ed29500e1b3f50225159a7b1e3d318d4409caad1955fe354f7f502363f%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   **[GPT-1 & 2: 预训练+微调带来的奇迹](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483710%26idx%3D1%26sn%3D7f9ccf321297e9e848be59a1b7aaacc5%26chksm%3Dec3688f8db4101ee72649c98bebe249f0e0b1e78dde6c7e09d35570c277c388bc6a0f28f038b%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   **[Bert: 双向预训练+微调](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483736%26idx%3D1%26sn%3Dc5116efdd7f935baccde0cd76bf05115%26chksm%3Dec36889edb410188ee2a269da9f2c14105a8f9afeab2d40ac01179a98e807f4760701cd27406%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   Bert与模型压缩

*   **[Bert与模型蒸馏：PKD和DistillBert](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483796%26idx%3D1%26sn%3Dc4acb3e8cf0cdc56d55943ef875e2f8c%26chksm%3Dec368852db410144f05119f0bef7d28b4e3de29d5b22c083c976d7ea93afd62d5fe5c01069a0%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   **[ALBert: 轻量级Bert](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483848%26idx%3D1%26sn%3D56c5a5bf0f61b0d0db226aa6e5fa210d%26chksm%3Dec36880edb4101189215fa8b9d68e9adf8d5659d16d5b1aca2354f19fdd0edb6eb07097319db%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   **[TinyBert: 模型蒸馏的全方位应用](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483867%26idx%3D1%26sn%3Dc83cb7109eea8d128d0e644baf8b2f30%26chksm%3Dec36881ddb41010b2df0afe1e866dda28f7a243f42104bce2499b29a206e0931dff0444274bf%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   **[MobileBert: Pixel4上只需40ms](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483931%26idx%3D1%26sn%3De3cba339cc6003fe549155ce9af37a87%26chksm%3Dec368bdddb4102cb1f9074e6fb024f1b5771e6c86184c2d1f406650f420f485e4e1fd998e304%26token%3D1357491502%26lang%3Dzh_CN%23rd)**


*   **[Transformer + AutoML: 进化的Transformer](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247484211%26idx%3D2%26sn%3Dcca62a5662dea8b3a24ccb2a75fa41f3%26chksm%3Dec368af5db4103e34b044ab5ad1e8a61b8e8517d4b0fd514d94126f6e4dd9e0a25faaaa522a5%26token%3D806322780%26lang%3Dzh_CN%23rd)**
*   Bert变种

*   **[Roberta: Bert调优](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483768%26idx%3D1%26sn%3Dc922c50c4154a295d2e18ebb468336be%26chksm%3Dec3688bedb4101a8fb5cc8fa8f86c01a8cd5e841fc00b850a435b412c323eee08f144b745b79%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   **[Electra: 判别还是生成，这是一个选择](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247484119%26idx%3D1%26sn%3D8391824e48fe3721204a0420aaa6f848%26chksm%3Dec368b11db4102078ec82f422c047e664f0fb9ce52ec5a78036b87ea7d491fa773e32943bc5d%26token%3D1355058864%26lang%3Dzh_CN%23rd)**
*   **[Bart: Seq2Seq预训练模型](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247484142%26idx%3D2%26sn%3De25a63b46cd4e54e8cace83cdad931fb%26chksm%3Dec368b28db41023eaa94252959057f6ea823ed0a25af46893c003ebb93540d337feaa0a0c9ce%26token%3D1106608718%26lang%3Dzh_CN%23rd)**
*   **[Transformer优化之自适应宽度注意力](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483910%26idx%3D1%26sn%3D850c743b140677e35557d148770b551c%26chksm%3Dec368bc0db4102d67774ac90605cdb1ea68fbe26aca675f4854748b40b87b401b5179a0f459f%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   **[Transformer优化之稀疏注意力](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247484022%26idx%3D1%26sn%3Df5367afb33952896a38a513320fdf17f%26chksm%3Dec368bb0db4102a664d6a867b905656a83d49ffaf163ccfffa77a09a8c48d711270a939d45aa%26token%3D176708162%26lang%3Dzh_CN%23rd)**
*   **[Reformer: 局部敏感哈希和可逆残差带来的高效](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483911%26idx%3D1%26sn%3D8d98a214d455a55650bb589830b08dae%26chksm%3Dec368bc1db4102d7d54216e917ec22f83b47df55153ef4c3f83aaafd68a0dd60caf836fb712a%26token%3D1357491502%26lang%3Dzh_CN%23rd)**
*   **[Longformer: 局部attentoin和全局attention的混搭](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483958%26idx%3D1%26sn%3Dde7b14bf4d2c56988598363dc12737fe%26chksm%3Dec368bf0db4102e6fd289bd8aa18df2748433a19c272e72946db64cacdad4d0bfb5d08abf90f%26token%3D1143207534%26lang%3Dzh_CN%23rd)**
*   **[Linformer: 线性复杂度的Attention](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247483972%26idx%3D1%26sn%3Dd9ae4489bd0d0a43e199aa96684906de%26chksm%3Dec368b82db41029438c8807e98384217f016540394f5e890423e9d0080caa0919a6ff91102ba%26token%3D1618065730%26lang%3Dzh_CN%23rd)**
*   **[XLM: 跨语言的Bert](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4ODg3NDY2NQ%3D%3D%26mid%3D2247484046%26idx%3D1%26sn%3D73634b99c940f96a4c9996c865f0173d%26chksm%3Dec368b48db41025eb1cc4a83aa54fa193e40428b70e47a1e5efa050de5d183387e807c5e73b7%26token%3D176708162%26lang%3Dzh_CN%23rd)**



